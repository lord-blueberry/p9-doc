\section{Discussion}\label{discussion}
In this project, we developed a novel $PSF$ approximation scheme. It leverages the fact that large parts of the $PSF$ from modern interferometers are close to zero, and we can use a sparse approximation of the true $PSF$. We then developed a parallel coordinate descent deconvolution algorithm that benefits from a sparse $PSF$. In our test, it achieved a speedup factor of 20 compared to the serial coordinate descent deconvolution algorithm, and an estimated speedup of factor 3 to CLEAN.

T

Our parallel coordinate descent algorithm is based on the APPROX\cite{fercoq2015accelerated}, which was extended to a distributed setting developed in \cite{fercoqfast}. In our project, we created a shared-memory implementation of the parallel coordinate descent algorithm. The same extension can be used to extend our parallel coordinate descent algorithm to a distributed setting. Distributing the parallel deconvolution algorithm is useful, if the whole image cannot be kept within the main memory of a single machine. However, it is not clear whether this is necessary.

In the self-calibration setting, intermediate solutions of the deconvolution are used to improve the calibration of the visibility measurements. New self-calibration methods account for distortions introduced by the Ionosphere, which change over the the image. This naturally splits the image into smaller facets, where each facet is deconvolved independently\cite{van2016lofar}. The total number and size of each facet depends on the objects in the observation, the science target and the radio interferometer itself. To our knowledge, it is currently not clear how large one can expect a facet to be.

The wall-clock time of our parallel coordinate descent algorithm scales with the size difference of the approximate $PSF$ and the total image size. In our tests, the parallel algorithm converged with an approximate $PSF$ which was $\frac{1}{32}$ of the total image size. If the image is split into smaller facets, then the size difference between the approximate $PSF$ and the facet is significantly smaller. Our parallel coordinate descent algorithm can still be used. If the approximate $PSF$ is larger than the facet, it may not be able to use all available processors effectively.

More effective with more concentrated $PSF$s.
With more data and larger baselines, we arrive at a $PSF$ which approximately Gaussian.
Works well for instruments like MeerKAT. also possible for future expansions, SKA-MID. 



%Our approximation used a window in the center of the $PSF$. Our parallel coordinate descent algorithm benefits from a smaller $PSF$ window. In our tests on a real-world MeerKAT observations, the parallel coordinate descent algorithm converged with window, where each side was $\frac{1}{32}$ of the total $PSF$ size. The $PSF$ window size is likely independent of the total image size. Meaning if we increase the field-of-view of the reconstruction, the approximated $PSF$ stays the same size. This in turn leads to faster parallel iterations, and can use more processors more efficiently.

Our serial and parallel coordinate descent algorithms used the elastic net regularization. It is a mixture between the L1- and L2-norms. On our test, the elastic net regularization lead to restored images which were comparable to state-of-the-art deconvolution algorithms like multi-scale CLEAN and MORESANE. The model images of the serial and parallel coordinate descent algorithms contained plausible structures, potentially super-resolving the N132D supernova-remnant. However, the model images of serial and parallel coordinate descent algorithms also seemed more susceptible towards calibration errors. This seems to be a general behavior of deconvolution algorithms based on convex optimization\footnote{In this project, we optimized a convex objective function ($\underset{x}{minimize} \: \frac{1}{2} \left \| I_{dirty} - x * PSF \right \|_2^2 + \lambda ElasticNet(x)$). CLEAN does optimizes a non-convex objective function, and is a is a non-convex optimization method.} \cite{offringa2017optimized}.

To our knowledge, there is currently little interest in elastic net as a regularization for radio interferometric imaging. Our results suggests elastic net may be an alternative to multi-scale CLEAN in terms of reconstruction quality, and for our parallel coordinate descent algorithm, also in terms of wall-clock time. Currently, the multi-scale CLEAN algorithm produces a residual image with lower pixel magnitudes, than our serial or parallel coordinate descent algorithm. Our algorithms leave a part of the true emission inside the residual image. This is at least in part due to the auto-masking strategies used in multi-scale CLEAN: After a certain number of iterations, multi-scale CLEAN is only allowed to change non-zero pixels in the model image. It is forbidden to add new sources to the model. An auto-masking strategy can also be implemented for serial and parallel coordinate descent.

%$\lambda$ parameter so far was leftto the user. Auto-thresholding may be used to estimate it.


May be improved with more strategies often used in Radio Interferometric imaging, like auto-masking.

First to use elastic net regularization. Simple model for extended emissions which has worked for a real-world MeerKAT observation.
Doe not need multi-scale stuff from CLEAN. 
Multi-scale CLEAN is slower than CLEAN.




Comparison of elasticNEt
Two regularizations are often used in radio astronomy: Starlets \cite{starck2015starlet} and Daubechies wavelets\cite{carrillo2014purify}. Both have been shown to produce super-resolved reconstruction in radio astronomy\cite{dabbech2015moresane, dabbech2018cygnus}. Elastic net is fairly simple. It is not known how elastic net now compares to the more complex regularization.
The optimal regularization in radio interferometric imaging is, to our knowledge, still an open question. 

Our parallel coordinate descent algorithm can also be used with different regularization, such as starlets and Daubechies wavelets. However, the parallel algorithm is fast, because it can exploit sparsity in the deconvolution problem (which we introduce with our $PSF$ approximation scheme). The elastic net regularization does not influence the sparsity of the $PSF$: It is a regularization which is only considers pixels independent of their neighborhood. The starlet regularization on the other hand considers neighborhoods of pixels of different sizes. For a large neighborhood, this diminishes the sparsity we introduced with our $PSF$ approximation scheme.  

Nevertheless, the starlet regularization also considers small neighborhoods, which can still be 'sparsified' with our $PSF$ approximation scheme, and potentially sped up with our parallel coordinate descent algorithm. It is unclear how our parallel coordinate descent algorithm with starlet regularization compares to MORESANE, the state-of-the-art reconstruction algorithm with starlet regularization.


e developed a proof-of-concept deconvolution algorithm, which only considers wide field-of-view imaging. A real-world reconstruction algorithm has to account for wide band imaging and different polarizations of the input data. This is currently ignored. Wide band imaging increases the problem: We cannot reconstruct one image, but have to reconstruct an image cube at different frequencies simultaneously. Furthermore, we introduce a regularization over the frequency.

It is not know for our parallel deconvolution algorithm, what effect does wide band imaging have. 
Sparsity, and a cheap single iteration.
But may be separated with respect to frequency with Lagrangian multipliers. Another dimension for parallelization. 
Question if cd methods are faster.


Potential of GPU. GPU is very good at atomic operations \cite{keplerShuffle}, which may speed our parallel coordinate descent algorithm. Other algorithms need GPU acceleration to be comparable to CLEAN \cite{dabbech2015moresane}, but our algorithm already is.


TT


We developed two deconvolution algorithms: A serial, and a parallel coordinate descent algorithm. The general experience is that deconvolution algorithms based on convex optimization are magnitudes slower than CLEAN\cite{offringa2017optimized}. The parallel coordinate descent algorithm together with our approximation methods our approximation method is magnitudes faster than the serial coordinate descent algorithm. On our estimates, the parallel algorithm and CLEAN have a comparable wall-clock time reconstructing a real-world observation. To our knowledge, our parallel algorithm is the only convex optimization based deconvolution algorithm in radio astronomy that has a comparable run time to CLEAN.

TT












W

Calibration and reconstruction is separate. In self-calibration, both calibration and image reconstruction are sovled together. We solve the image reconstruction problem several times.
Huge can of worms, due to the self-calibration bias.
May introduce facetting, a natural way to distribute the problem in image space.

Automation, we do not want to specify a $\lambda$
Very differnt.
Our implicit path regularization may help there.


Works well for MeerKAT. Probably not for LOFAR. MeerKAT has the small $PSF$










How the $PSF$ approximation scheme works for other 
A simple regularization. It is not known how it compares to more complex 


 , which can speed benefits from a sparse $PSF$ With the  We developed a parallel deconvolution algorithm which benefits
Approximation
And A coordinate descent algorithm that can  exploit it.

Main thing: parallel coordinate descent works.
Times comparable to standard CLEAN.
Reconstruction quality similar, or superior to serial coordinate descent
Super-resolution..
One more major cycle.
Comparable number of major cycles to CLEAN. Exact comparison is difficult

Due to our gradient approximation scheme. We exploit the fact that the $PSF$ of interferometers like MeerKAT is fairly concentrated around the center.
Exploited by the parallel coordinate descent algorithm. 
Serial coordinate descent was difficult to speedup, even with GPU and distributed reconstruction.

Easy to extend to a distributed setting, hydra.

Expect it to scale better with larger field of views. The larger the field of view, the more concentrated the $PSF$ is in relation to the whole image.
It is not clear, whether we introduce a systematic error with the approximation. We couldn't find one.

Serial coordinate descent
Similar to the standard CLEAN.
In our implementation, it did not benefit a lot from GPU acceleration. Only by a factor of 2. Unclear if this generalizes to multi-scale CLEAN.

CLEAN
CLEAN is striclty better with calibration error.
Better residuals. Compared to coordinate descent methods, it manages small iteration counts.
Our coordinate descent methods achieved super-resolution. Artificial example, but still.

Question about regularization
Used the elasticNet regularization. Cheap and easy. More sophisticated, 

Multiy frequency.



Works well for MeerKAT. Its $PSF$ is located many not work well for LOFAR.

Difficult to achieve speedup with a dense $PSF$ we approximated












\subsection{Multi frequency extension}\label{discussion:mfs}
Difficult.

Regularized inverse problem  \cite{ferrari2015multi}. Objective function 
How it works, adding a new term to the objective function

\begin{equation}\label{cd:deconv}
\underset{x}{minimize} \: \frac{1}{2} \left \| I_{dirty} - X * PSF \right \|_2^2 + \lambda ElasticNet(X) + \lambda_v \left \| DX \right \|_1
\end{equation}

Where $D$ is the Discrete cosine transform.

Does not have a proximal operator for each pixel. problem for Coordinate descent method.

Question if each iteration can be cheap.

But may be separated with respect to frequency with Lagrangian multipliers. Question if cd methods are faster.
