\section{Discussion}\label{discussion}
In this project, we developed a novel $PSF$ approximation scheme. It leverages the fact that large parts of the $PSF$ from modern interferometers are close to zero, and we can use a sparse approximation of the true $PSF$. We then developed a parallel coordinate descent deconvolution algorithm that benefits from a sparse $PSF$.  We were able to create a non-blocking parallel implementation, which achieved a speedup factor of 20 compared to the serial coordinate descent deconvolution algorithm, and an estimated speedup of factor 3 to CLEAN.

T

High throughput of parallel deconvolutions.
The degree of parallelism, the number of parallel deconvolutions which can be used efficiently, scales with both the field-of-view of the observation, and the approximate $PSF$. With larger field of views, and with smaller approximated $PSF$s, the parallel coordinate descent algorithm can efficiently use a larger number of processors. 

Scales with the problem size.
GPU acceleration
This algorithm has the potential to scale with the planned expansion of MeerKAT to SKA-Mid.
Has some unclear behaviour

SKA-Mid

In our project, we created a shared-memory implementation of the parallel coordinate descent algorithm. Our parallel algorithm is based on the APPROX\cite{fercoq2015accelerated}, which was extended to a distributed setting developed in \cite{fercoqfast}. The same extension can be used to extend our parallel coordinate descent algorithm to a distributed setting. Distributing the parallel deconvolution algorithm is useful, if the whole image cannot be kept within the main memory of a single machine. However, it is not clear whether this is necessary.

In the self-calibration setting, intermediate solutions of the deconvolution are used to improve the calibration of the visibility measurements. New self-calibration methods account for distortions introduced by the Ionosphere, which change over the the image. This naturally splits the image into smaller facets, where each facet is deconvolved independently\cite{van2016lofar}. The total number and size of each facet depends on the objects in the observation, the science target and the radio interferometer itself. To our knowledge, it is currently not clear how large one can expect a facet to be.

Our parallel coordinate descent algorithm scales with the amount of sparsity our approximation method can introduce. In our tests, the parallel algorithm converged with an approximate $PSF$ which was $\frac{1}{32}$ of the total image size. If the image is split into smaller facets, then the size difference between the approximate $PSF$ and the facet is significantly smaller. If the approximate $PSF$ is larger than the facet, the parallel algorithm may not be able to use all available processors effectively. On the other hand, if the approximate $PSF$ is smaller than the facet, our parallel algorithm can significantly speed up the deconvolution. The smaller the approximate $PSF$ is compared to the facet, the more processors can be used efficiently, and our parallel coordinate descent algorithm can be faster to converge than multi-scale CLEAN.

The speedup achieved by the parallel coordinate descent method depends on the size of the image to deconvolve, and the size of the approximate $PSF$. Self-calibration with facets may effectively reduce the image size. But observations which need self-calibration tend to have a large amount of visibility measurements, which in turn may also lead to a smaller approximate $PSF$. The $PSF$ approaches a 2d Gaussian with increasing number of visibility measurements. This reduces the error our $PSF$ approximation introduces. With more visibility measurements, it may produce a smaller approximate $PSF$ with a similar error. The parallel coordinate descent algorithm in turn can achieve higher speedups with a smaller approximate $PSF$. If the parallel coordinate descent algorithm can beat multi-scale CLEAN in a self-calibration task has to be evaluated in the future.

Different instruments
Lower speedup for instruments like LOFAR. 
Our approximation scheme exploits the fact that instruments are often nearly a gaussian function. This may not work for instruments with lower frequencies, like LOFAR.


%Our approximation used a window in the center of the $PSF$. Our parallel coordinate descent algorithm benefits from a smaller $PSF$ window. In our tests on a real-world MeerKAT observations, the parallel coordinate descent algorithm converged with window, where each side was $\frac{1}{32}$ of the total $PSF$ size. The $PSF$ window size is likely independent of the total image size. Meaning if we increase the field-of-view of the reconstruction, the approximated $PSF$ stays the same size. This in turn leads to faster parallel iterations, and can use more processors more efficiently.

Our serial and parallel coordinate descent algorithms used the elastic net regularization. It is a mixture between the L1- and L2-norms. On our test, the elastic net regularization lead to restored images which were comparable to state-of-the-art deconvolution algorithms like multi-scale CLEAN and MORESANE. The model image resulting from elastic net contained plausible structures, potentially super-resolving the N132D supernova-remnant. However, the model images resulting from elastic net also seemed more susceptible towards calibration errors. This seems to be a general behavior of deconvolution algorithms based on convex optimization\footnote{In this project, we optimized a convex objective function ($\underset{x}{minimize} \: \frac{1}{2} \left \| I_{dirty} - x * PSF \right \|_2^2 + \lambda ElasticNet(x)$). CLEAN does optimizes a non-convex objective function.} \cite{offringa2017optimized}.

Our results suggests elastic net may be an alternative to multi-scale CLEAN in terms of reconstruction quality, and combined with the parallel coordinate descent algorithm, also in terms of wall-clock time. Our current implementation has one main drawback compared to multi-scale CLEAN algorithm: It leaves  a part of the true emission inside the residual image. The pixel magnitudes of the residual image are significantly larger than those of the multi-scale CLEAN residual image. This is at least in part due to the auto-masking strategies used in multi-scale CLEAN: After a certain number of iterations, multi-scale CLEAN is only allowed to change non-zero pixels in the model image. It is forbidden to add new sources to the model. Our serial and parallel coordinate descent algorithm can be extended with a similar auto-masking strategy to CLEAN. This is a necessary extension for any future parallel coordinate descent implementation.

To our knowledge, there is currently little interest in elastic net as a regularization for radio interferometric imaging. Currently, over-complete representations are often used as a regularization. Two frequently used examples are the Starlet \cite{starck2015starlet} and Daubechies wavelet regularization \cite{carrillo2014purify}. Both have been shown to produce super-resolved reconstruction in radio astronomy\cite{girard2015sparse, dabbech2018cygnus}. Compared to the over-complete regularizations. The parallel coordinate descent algorithm is not limited to the elastic net regularization. It can be extended to use Starlets or Daubechies wavelets. However, the parallel algorithm achieves its speedup by exploiting sparsity in the approximate $PSF$. Elastic net is a pixel-wise regularization, and does not affect the sparsity in the approximate $PSF$. A regularization like Starlets affects a neighborhood of pixels, which in turn reduces the sparsity in the approximate $PSF$. The larger the neighborhood, the larger is the combined $PSF$s of all pixels. Nevertheless, depending on the size of the neighborhood there may still be enough sparsity to exploit with the parallel coordinate descent algorithm. If this leads to a faster algorithm than MORESANE (which also uses the starlet regularization) has to be evaluated in future works.

The implementation of our parallel coordinate descent algorithm does not account for wide-band imaging effects, which is the main limitation of our work. In wide-band imaging, the a deconvolution algorithm has to reconstruct three dimensional image cube, where each image represents a single sub-band. Additionally, a wide-band deconvolution algorithm needs a regularization across the frequency axis \cite{ferrari2015multi}. Wide-band imaging allows for another dimension of parallelization \cite{ferrari2015multi}. The parallel coordinate descent algorithm was able to efficiently use multiple processors for deconvolving a single image. In our case, wide-band imaging can potentially lead to a higher degree of parallelism.

The parallel coordinate descent algorithm was tested on a narrow-band imaging problem. It is unknown how it compares to state-of-the-art wide-band imaging algorithms like multi-scale, multi-frequency-synthesis CLEAN (MS-MFS-CLEAN). Interferometers like MeerKAT measure visibilities in 20'000 channels simultaneously. A wide-band deconvolution algorithm is necessary for leveraging the full potential of MeerKAT measurements. The next step for the parallel coordinate descent algorithm is to extend it to the wide-band imaging case, and compare it to state-of-the-art MS-MFS-CLEAN algorithm.
 

%$\lambda$ parameter so far was leftto the user. Auto-thresholding may be used to estimate it.


%Our parallel coordinate descent algorithm can also be used with different regularization, such as starlets and Daubechies wavelets. However, the parallel algorithm is fast, because it can exploit sparsity in the deconvolution problem (which we introduce with our $PSF$ approximation scheme). The elastic net regularization does not influence the sparsity of the $PSF$: It is a regularization which is only considers pixels independent of their neighborhood. The starlet regularization on the other hand considers neighborhoods of pixels of different sizes. For a large neighborhood, this diminishes the sparsity we introduced with our $PSF$ approximation scheme.  

%Nevertheless, the starlet regularization also considers small neighborhoods, which can still be 'sparsified' with our $PSF$ approximation scheme, and potentially sped up with our parallel coordinate descent algorithm. It is unclear how our parallel coordinate descent algorithm with starlet regularization compares to MORESANE, the state-of-the-art reconstruction algorithm with starlet regularization.


%Potential of GPU. GPU is very good at atomic operations \cite{keplerShuffle}, which may speed our parallel coordinate descent algorithm. Other algorithms need GPU acceleration to be comparable to CLEAN \cite{dabbech2015moresane}, but our algorithm already is.

